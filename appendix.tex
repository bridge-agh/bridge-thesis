\uchapter{\ChapterTitleAppendix}
\label{sec:appendix}

\begin{algorithm}
    \caption{Pętla główna uczenia AlphaZero}\label{alg:alphazero-main}
    \KwOut{Learned weights $\theta$}
    $\theta \gets$ initial random weights\;
    Collect observations from random-move games to initialize batch norm stats\;
    experience\_buffer $\gets$ empty list\;
    \Repeat{training budget exhausted}{
        \For{$i \gets 1$ \KwTo self-play games per epoch}{
            $T \gets$ self\_play($\theta$)\;
            $X \gets$ process\_trajectory($T$)\;
            experience\_buffer $\gets$ experience\_buffer $\cup$ $X$\;
        }
        Truncate experience\_buffer to fixed size with most recent data\;
        \For{$i \gets 1$ \KwTo training iterations}{
            $X \gets$ sample batch from experience\_buffer without replacement\;
            $\theta \gets$ train\_step($\theta$, $X$)\;
        }
    }
    \Return $\theta$\;
\end{algorithm}

\begin{algorithm}
    \caption{Procedura self\_play}\label{alg:alphazero-self-play}
    \KwIn{Weights $\theta$}
    \KwOut{Trajectory $T$}
    $T \gets$ empty list\;
    $s \gets$ random initial game state\;
    \While{$s$ is not terminal}{
        \tcc{See algorithm in \cite{GumbelAZ}}
        $\pi \gets$ GumbelAlphaZeroPolicy($s$, $\theta$)\;
        $a \gets$ GumbelRootActionSelection($\pi$)\;
        $s' \gets$ $s$.apply($a$)\;
        $s$.action\_weights $\gets$ $\pi$\;
        $T \gets T \cup \{s\}$\;
        $s \gets s'$\;
    }
    \Return $T$\;
\end{algorithm}

\begin{algorithm}
    \caption{Procedura process\_trajectory}\label{alg:alphazero-process-trajectory}
    \KwIn{Trajectory $T$}
    \KwOut{Batch of training data $X$}
    $X \gets$ empty list\;
    \ForEach{state $s$ in $T$}{
        $x \gets$ empty example\;
        $x$.observation $\gets$ $s$.observation\;
        $x$.policy\_target $\gets$ $s$.action\_weights\;
        $x$.value\_mask $\gets$ $1$ if $T[\mathrm{end}]$ is terminal, $0$ otherwise\;
        $x$.value\_target $\gets \begin{cases}
                1  & \text{if}\ T[\mathrm{end}].\mathrm{winner} = s.\mathrm{player}                 \\
                -1 & \text{if}\ T[\mathrm{end}].\mathrm{winner} = \mathrm{other}(s.\mathrm{player}) \\
                0  & \text{otherwise}
            \end{cases}$\\
        $X \gets X \cup \{x\}$\;
    }
    \Return $X$\;
\end{algorithm}

\begin{algorithm}
    \caption{Procedura train\_step}\label{alg:alphazero-train-step}
    \KwIn{Weights $\theta$, batch of training data $X$}
    \KwOut{Updated weights $\theta$}
    \ForEach{example $x$ in $X$}{
        $z \gets$ $x$.value\_target\;
        $m \gets$ $x$.value\_mask\;
        $\pi \gets$ $x$.policy\_target\;
        $v, \mathbf{p} \gets$ model($x$.observation, $\theta$)\;
        $\mathcal{L} \gets m(z - v)^2 - \pi^T \log \mathbf{p} + c ||\theta||^2$\;
        $\theta \gets$ adam\_optimizer\_step($\theta$, $\mathcal{L}$)\;
    }
    \Return $\theta$\;
\end{algorithm}

\begin{table}
    \centering
    \caption{Parametry algorytmu Gumbel AlphaZero}
    \label{tab:gumbel-alphazero-params}
    \begin{tabular}{ll}
        \toprule
        Parametr & Wartość \\ \midrule
        Gumbel scale \cite{GumbelAZ} & 1.0 \\
        MCTS max depth & 64 \\
        MCTS simulations & 32 \\
        $Q$-value transform & Completed by mix value \cite{GumbelAZ}; Appendix D \\
        Self-play iterations per epoch & 8 \\
        Self-play batch size & 256 \\
        Training iterations per epoch & 16 \\
        Training batch size & 8192 \\
        Experience buffer size & 1e6 \\
        Optimizer & Adam \cite{Adam} \\
        Learning rate & 1e-3 \\
        Regularization constant $c$ & 1e-4 \\
        Epochs & 846 \\
        Training time & 21h 26m \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}
    \centering
    \caption{Architektura sieci neuronowej}
    \label{tab:nn-architecture}
    \begin{tabular}{ll}
        \toprule
        Warstwa & Rozmiar \\ \midrule
        Input & (480) \\
        Linear & (4, 4, 32) \\
        \midrule
        \multicolumn{2}{c}{Initial convolution} \\
        Conv2D 3x3 & (4, 4, 64) \\
        BatchNorm & (4, 4, 64) \\
        ReLU & (4, 4, 64) \\
        \midrule
        \multicolumn{2}{c}{Residual block $\times 9$} \\
        Conv2D 3x3 & (4, 4, 64) \\
        BatchNorm & (4, 4, 64) \\
        ReLU & (4, 4, 64) \\
        Conv2D 3x3 & (4, 4, 64) \\
        BatchNorm & (4, 4, 64) \\
        Residual connection & (4, 4, 64) \\
        ReLU & (4, 4, 64) \\
        \midrule
        \multicolumn{2}{c}{Policy head} \\
        Conv2D 1x1 & (4, 4, 2) \\
        BatchNorm & (4, 4, 2) \\
        ReLU & (4, 4, 2) \\
        Linear & (38) \\
        \midrule
        \multicolumn{2}{c}{Value head} \\
        Conv2D 1x1 & (4, 4, 1) \\
        BatchNorm & (4, 4, 1) \\
        ReLU & (4, 4, 1) \\
        Linear & (256) \\
        ReLU & (256) \\
        Linear & (1) \\
        Tanh & (1) \\
        \bottomrule
    \end{tabular}
\end{table}

% 0 -> 0 = 1
% 0 -> 1 = -0.5
% 1 -> 0 = -1
% 1 -> 1 = 1

% Wins: 0.53, Draws: 0.00, Losses: 0.47
% [[1.         0.        ]
%  [0.14       0.85999995]]
